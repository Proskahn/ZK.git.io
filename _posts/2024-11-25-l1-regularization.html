---

layout: post
title: Does L1-regularization really promote sparsity?
subtitle:
date: 2024-11-24
background: '/img/posts/Figure5.png'
---

When I looked through web pages about L1-regularization, I found that many people claim L1 induces sparsity in neural networks. However, I also came across some posts on Stack Overflow asking, “Why doesn’t L1-regularization force the weight parameters to become zero?”

As part of my coursework, I investigated this topic through research and experimentation. If you were to ask me, “Does L1-regularization really reduce the number of non-zero parameters?”

My answer is: **I do not think so.**


Click the link below to read the full article.

https://medium.com/@k13659082460/does-l1-regularization-really-promote-sparsity-dc300a54c413